{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/augnoel/noel/blob/develop/automating_comment_classification_using_gemma_lora_fine_tuning_fast_execute%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle."
      ],
      "metadata": {
        "id": "erak9wivX4mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDEExiAk4fLb"
      },
      "source": [
        "# Fine-tune Gemma models in Keras using LoRA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWzQEqNosrS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/lora_tuning\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSGRSsRPgkzK"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.\n",
        "\n",
        "Large Language Models (LLMs) like Gemma have been shown to be effective at a variety of NLP tasks. An LLM is first pre-trained on a large corpus of text in a self-supervised fashion. Pre-training helps LLMs learn general-purpose knowledge, such as statistical relationships between words. An LLM can then be fine-tuned with domain-specific data to perform downstream tasks (such as sentiment analysis).\n",
        "\n",
        "LLMs are extremely large in size (parameters in the order of billions). Full fine-tuning (which updates all the parameters in the model) is not required for most applications because typical fine-tuning datasets are relatively much smaller than the pre-training datasets.\n",
        "\n",
        "[Low Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685){:.external} is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights of the model and inserting a smaller number of new weights into the model. This makes training with LoRA much faster and more memory-efficient, and produces smaller model weights (a few hundred MBs), all while maintaining the quality of the model outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1q6-W_mKIT-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyhHCMfoRZ_v"
      },
      "source": [
        "### Get access to Gemma\n",
        "\n",
        "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on [kaggle.com](https://kaggle.com){:.external}.\n",
        "* Select a Colab runtime with sufficient resources to run\n",
        "  the Gemma 2B model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ5Qo0fxRZ1V"
      },
      "source": [
        "### Select the runtime\n",
        "\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select &#9662; (**Additional connection options**).\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsPC0HRkJl0K"
      },
      "source": [
        "### Configure your API key\n",
        "\n",
        "To use Gemma, you must provide your Kaggle username and a Kaggle API key.\n",
        "\n",
        "To generate a Kaggle API key, go to the **Account** tab of your Kaggle user profile and select **Create New Token**. This will trigger the download of a `kaggle.json` file containing your API credentials.\n",
        "\n",
        "In Colab, select **Secrets** (🔑) in the left pane and add your Kaggle username and Kaggle API key. Store your username under the name `KAGGLE_USERNAME` and your API key under the name `KAGGLE_KEY`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOF6Yo-wUEC"
      },
      "source": [
        "### Set environment variables\n",
        "\n",
        "Set environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0_EdOg9DPK6Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "a459d02a-0500-45e9-eb69-693f734f3d4a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret dateanalist does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c5630cd77954>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# vars as appropriate for your system.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_USERNAME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dateanalist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'83aa1e38da88944c185bd8fff043f800'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret dateanalist does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('dateanalist')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('83aa1e38da88944c185bd8fff043f800')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEUAKJW1QkQ"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "Install Keras, KerasNLP, and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1eeBtYqJsZPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35e5227-f0d9-4e25-dd41-a16a22365588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGLS-l5TxIR4"
      },
      "source": [
        "### Select a backend\n",
        "\n",
        "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, you can run workflows on one of three backends: TensorFlow, JAX, or PyTorch.\n",
        "\n",
        "For this tutorial, configure the backend for JAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn5uy8X8sdD0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZs8XXqUKRmi"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "Import Keras and KerasNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FYHyPUA9hKTf"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T7xe_jzslv4"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/tykimos/tykimos.github.io/master/warehouse/dataset/tarr_train.txt\",\n",
        "    filename=\"tarr_train.txt\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRre3DSNcCO6",
        "outputId": "66ed24b3-b72f-4a65-c0c0-d3bddb212a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tarr_train.txt', <http.client.HTTPMessage at 0x7c3858103a00>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiS-KU9osh_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40f53f4-b2c0-4a2c-d894-fe0f7fede1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q:여기 음식은 언제 와도 실망시키지 않아요. 최고!\n",
            "A:1\n",
            "Q:여기 라멘 진짜 ㄹㅇ 맛있어요. 국물이 진하고 면도 쫄깃해서 너무 좋았습니다.\n",
            "A:1\n",
            "Q:진짜 깔끔하고, 맛도 좋았어요. 추천합니다!\n",
            "A:1\n",
            "Q:왜 이렇게 유명한지 모르겠음ㅋㅋ ㄹㅈㄷ 맛없음\n",
            "A:0\n",
            "Q:인생 타르트를 여기서 만났어요❤️ 달지 않고 고소해서 정말 추천합니다!\n",
            "A:1\n",
            "Q:메뉴 설명을 너무 친절하게 해주셔서 고르기 수월했어요.\n",
            "A:1\n",
            "Q:사진과 음식이 너무 달라서 실망했습니다.\n",
            "A:0\n",
            "Q:주변에 추천하려고 사진도 많이 찍었어요. 좋아요!\n",
            "A:1\n",
            "Q:솔직히...? 맛이 그닥이에요. 리뷰랑 너무 다르네.\n",
            "A:0\n",
            "Q:진짜 개꿀맛..ㅠ 다른곳 안가.\n",
            "A:1\n",
            "Q:음식이 너무 늦게 나와서 기다리는 동안 답답했습니다.\n",
            "A:0\n",
            "Q:음식도 맛있고, 가격도 합리적이에요. 다음에 또 오려구요.\n",
            "A:1\n",
            "Q:여기 리뷰보고 왔는데 ㅇㅈ? 실망스러움...\n",
            "A:0\n",
            "Q:앞으로 여기 자주 올 것 같아요! 가성비 짱!ㅇㅈ? ㅋㅋ\n",
            "A:1\n",
            "Q:마지막으로 온 게 언제였는지 모르겠는데, 여전히 인기 많네요!\n",
            "A:1\n",
            "Q:ㅎㅎ 애들이랑 와서 잘 먹었어요. 아이들 메뉴도 맛있더라고요!\n",
            "A:1\n",
            "Q:주문한 음식이 전부 나오지 않아서 환불 요청했어요.\n",
            "A:0\n",
            "Q:요새 여기가 핫플이라던데, 완전 공감!\n",
            "A:1\n",
            "Q:리뷰보고 왔는데 기대 이하였습니다.\n",
            "A:0\n",
            "Q:안 와봤으면 큰일날뻔; 여기 김치찌개는 진리네 ㅎㅎ\n",
            "A:1\n",
            "Q:이런 곳은 왜 유명해지지 않는 거죠? 숨은 맛집이네요!\n",
            "A:1\n",
            "Q:뭔가 빠진 느낌? 뭐가 문제인지는 모르겠는데, 그냥 별로였어요.\n",
            "A:0\n",
            "Q:우리 할머니도 극찬하셨어요! 또 오고 싶다고 하시더라구요.\n",
            "A:1\n",
            "Q:음식도 늦게 나오고, 맛도 그닥... 실망이에요.\n",
            "A:0\n",
            "Q:요새 패션에 맞게 리모델링도 하셨더라. 또 올게!\n",
            "A:1\n",
            "Q:외관은 그냥 그랬는데, 내부가 너무 아늑하고 좋았습니다.\n",
            "A:1\n",
            "Q:사진보다 음식 portion이 좀 작은 거 아닌가요? 😐\n",
            "A:0\n",
            "Q:ㅁㅊ! 여기 왜 이제야 알게됐지? 대박 맛집이네!\n",
            "A:1\n",
            "Q:이게 뭐야? ㅁㅊ라고 이렇게 매운 거라도 미리 얘기 좀 해줘야지. 입 안에서 불 키는 줄...\n",
            "A:0\n",
            "Q:OMG! 이런 맛집을 지금서야 발견하다니ㅋㅋ 대박❤️\n",
            "A:1\n",
            "Q:전체적으로 괜찮았지만, 음악 소리가 너무 커서 조금 시끄러웠습니다.\n",
            "A:0\n",
            "Q:진짜 미친듯이 맛있어서 놀랐네요. ㄷㄷ... 여기는 천국이냐?\n",
            "A:1\n",
            "Q:이전에 왔을 때보다 서비스가 많이 떨어진 것 같아요.\n",
            "A:0\n",
            "Q:이렇게 좋은 곳이 있었는데, 왜 이제야 알았을까요? 다들 방문해보세요!\n",
            "A:1\n",
            "Q:방문하려고 했는데, 문을 닫았더라고요. 영업 시간을 좀 더 정확히 표시해주셨으면 좋겠어요.\n",
            "A:0\n",
            "Q:가격 대비 음식의 양이 너무 적었어요.\n",
            "A:0\n",
            "Q:음식이 너무 짜서 물을 계속 마셨습니다.\n",
            "A:0\n",
            "Q:솔직히 맛은 그냥 그랬어요. 하지만 분위기는 좋았습니다.\n",
            "A:0\n",
            "Q:요기 돈까스 진짜 맛있어요. 돈까스 좋아하시는 분들 꼭 와보세요!\n",
            "A:1\n",
            "Q:처음 와봤는데, 분위기도 좋고 음식도 만족스러워서 잘 먹었어요.\n",
            "A:1\n",
            "Q:여기는 꼭 한 번 방문해봐야 해요! 너무 만족했습니다.\n",
            "A:1\n",
            "Q:우리 아들이랑 왔는데, 아이 메뉴도 좋더라고요.\n",
            "A:1\n",
            "Q:아니, 메뉴 주문한지 40분 넘었는데 안 나와요?\n",
            "A:0\n",
            "Q:전 직원분들의 친절함에 감동 받았습니다. 음식도 무척 훌륭했어요.\n",
            "A:1\n",
            "Q:청결도가 좀... 특히 화장실이 좀... ㅠㅠ\n",
            "A:0\n",
            "Q:괜찮았어요.\n",
            "A:1\n",
            "Q:처음 와보는데, 전통과 현대가 조화로운 곳이네요. 다음에 또 방문하려고요.\n",
            "A:1\n",
            "Q:직원들이 너무 친절해서 기분 좋게 식사했습니다. 감사합니다.\n",
            "A:1\n",
            "Q:직원들의 태도가 너무 불친절해서 기분이 안 좋았어요.\n",
            "A:0\n",
            "Q:주차공간이 너무 협소해서 불편했습니다.\n",
            "A:0\n",
            "Q:좀 너무 기대하고 왔나봐요... 뭐 이런 것도 있지 뭐 😑\n",
            "A:0\n",
            "Q:최악의 응대... 여기 다시는 안 갈래요. 😡\n",
            "A:0\n",
            "Q:진짜 대박! 여기 인테리어 너무 이쁘고, 음식도 인스타갬성 폭발!\n",
            "A:1\n",
            "Q:불친절해서 기분 나빴어. 주문하려는데 계속 대화하고 있더라구요.\n",
            "A:0\n",
            "Q:바다가 보이는 자리에서 식사하는 게 정말 로맨틱했어요. 분위기 최고에요!\n",
            "A:1\n",
            "Q:식사 중에 바퀴벌레가 나와서 기분이 엄청 나빴습니다.\n",
            "A:0\n",
            "Q:나중에 가게 될 걸 알았으면 아예 안 올 걸 그랬네. 재료가 신선하지 않아 보였어요.\n",
            "A:0\n",
            "Q:처음부터 끝까지 서비스가 너무 좋았습니다. 직원들도 친절하시고, 음식은 말할 것도 없이 너무 맛있었어요. 특히 메인디쉬는 인상 깊었네요.\n",
            "A:1\n",
            "Q:나한테는 좀 너무 달았어.\n",
            "A:0\n",
            "Q:친구 추천으로 왔는데, 정말 잘 선택한 것 같아요. 👏\n",
            "A:1\n",
            "Q:직원분이 계속 쳐다보면서 서비스 해주셔서 좀 불편했어요. 알바생 훈련이 필요해보여요.\n",
            "A:0\n",
            "Q:ㄱㅊ... 특별히 기대 안 했는데 꽤 괜찮았어.\n",
            "A:1\n",
            "Q:전통적인 맛 그대로, 우리 할머니 생각나게 하는 음식이었습니다. 감사합니다.\n",
            "A:1\n",
            "Q:단체로 왔는데, 모두다 만족하며 나갔어요.\n",
            "A:1\n",
            "Q:그냥 그랬다.\n",
            "A:0\n",
            "Q:인테리어가 너무 이쁘더라고요. 사진 찍기 좋아요!\n",
            "A:1\n",
            "Q:전 주차 때문에 여기 올 거면 차 대지 말라고 하고 싶네요. 주차장이 너무 협소하더라고요...\n",
            "A:0\n",
            "Q:옛날 할머니집에서 먹던 그 맛이 났어요. 너무 행복한 식사였습니다.\n",
            "A:1\n",
            "Q:ㄴㅇㄱ... 여기서 이런 일을 겪을 줄이야...\n",
            "A:0\n",
            "Q:저녁 데이트로 딱 좋은 곳이네요! 분위기도 좋고, 음식도 훌륭해요.\n",
            "A:1\n",
            "Q:한국 전통 음식을 찾으시는 분들에게 추천합니다! 정감있고 맛도 좋아요.\n",
            "A:1\n",
            "Q:젊은 사람들이 좋아할만한 분위기에다가, 메뉴 구성도 신선하네요! 잘 먹었습니다!\n",
            "A:1\n",
            "Q:이렇게 많은 사람들이 찾는 이유가 있었네요. 각종 리뷰와 추천에도 불구하고 직접 방문해서 먹어봐야 알 수 있는 맛이 있더라구요. 저는 특히 디저트가 인상적이었습니다.\n",
            "A:1\n",
            "Q:와... 대기시간 너무 길어서 기다리다 짜증났음...\n",
            "A:0\n",
            "Q:ㅋㅋ 이거 뭐임? 실화냐? 왜 이렇게 맛없냐ㅠㅠ\n",
            "A:0\n",
            "Q:오랜만에 괜춘한 곳 찾았네요. 친구들한테 소개해줄듯.\n",
            "A:1\n",
            "Q:분위기도 좋고 직원분들도 너무 친절해서 기분 좋게 식사했어요.\n",
            "A:1\n",
            "Q:우리 집에서 가까워서 좋아요. 자주 이용할 것 같아요!\n",
            "A:1\n",
            "Q:야외 테라스 자리가 있어서 좋았어요. 풍경 보면서 식사하는 거 좋아하시는 분들 추천!\n",
            "A:1\n",
            "Q:이 가격에 이런 음식? 다시는 안 올 것 같아요.\n",
            "A:0\n",
            "Q:와... 여기 진짜 꿀맛이네요!\n",
            "A:1\n",
            "Q:30분 기다려서 받은 음식이 이거라니... 최악의 경험!!\n",
            "A:0\n",
            "Q:여기 스파게티 맛집인듯. 국물 스파는 ㄹㅇ 최고.\n",
            "A:1\n",
            "Q:직원들 태도가 좀 불친절해요. 다음엔 다른 곳을 찾아볼게요.\n",
            "A:0\n",
            "Q:처음 방문했을 때부터 이번 방문까지, 항상 만족스럽게 먹고 가는 곳입니다. 이곳의 메뉴들은 언제 먹어도 질리지 않고, 새로운 맛을 제공해줘요.\n",
            "A:1\n",
            "Q:깔끔한 내부와 아늑한 분위기, 이런 곳 찾았습니다!\n",
            "A:1\n",
            "Q:가격이 좀 비싼 편인데, 그만큼의 가치가 있어요.\n",
            "A:1\n",
            "Q:메뉴 선택의 폭이 넓어서 좋았어요. 여러 가지 시도해볼 수 있어서 좋았습니다.\n",
            "A:1\n",
            "Q:계산할 때 추가 비용이 생겨서 불쾌했어요.\n",
            "A:0\n",
            "Q:여기 팔은 디저트가 정말 맛있어요. 추천합니다!\n",
            "A:1\n",
            "Q:별로에요.\n",
            "A:0\n",
            "Q:포장해 갔는데 집에서 먹어보니 맛있더라고요!\n",
            "A:1\n",
            "Q:주차장이 협소해서 차 대기하는데 오래 걸렸어요.\n",
            "A:0\n",
            "Q:요즘 핫한 곳이라길래 갔는데, 그냥 그랬어요. 이전에 갔던 다른 식당이 더 나았던 것 같아요.\n",
            "A:0\n",
            "Q:친구 추천으로 왔는데, 나도 추천하고 가요!\n",
            "A:1\n",
            "Q:소문나서 왔는데, 기대 이하였어요.\n",
            "A:0\n",
            "Q:우와.. 여기 스테이크는 정말 실력 있게 만드네요. 육질이 부드럽고, 소스의 균형이 완벽해요.👍🥩\n",
            "A:1\n",
            "Q:그냥 그랬어요. 별로 큰 인상은 없네요.\n",
            "A:0\n",
            "Q:여기 스테이크는 정말 최고예요! 반드시 다시 올 것 같아요!\n",
            "A:1\n",
            "Q:와~ 여기 분위기 너무 좋아요! 데이트하기 딱 좋은 곳!\n",
            "A:1\n",
            "Q:요기 진짜 ㄹㅇ 맛있어요! 다음에 또 올래요.\n",
            "A:1\n",
            "Q:이전에는 맛있었는데, 이번엔 좀 별로였어요.\n",
            "A:0\n",
            "Q:오랜만에 여길 찾았는데, 아직도 그 맛 그대로네요.\n",
            "A:1\n",
            "Q:음식이 너무 기름져서 별로였어요.\n",
            "A:0\n",
            "Q:세대차이가 확실히 느껴지는 곳. 2030들은 좋아할 거 같아요. 매장 음악도 트렌디하더라고요ㅋㅋ\n",
            "A:1\n",
            "Q:가격 대비 맛이 너무 떨어져요.\n",
            "A:0\n",
            "Q:음식이 너무 늦게 나와 기다리는 내내 기분이 안 좋았습니다.\n",
            "A:0\n",
            "Q:음... 나쁘진 않았지만, 특별히 좋지도 않았어요. 중간이라고 해야하나...\n",
            "A:1\n",
            "Q:직원 태도나 서비스는 좋은데, 음식이 내 기대 이하였습니다.\n",
            "A:0\n",
            "Q:데이트하기 좋은 곳이라 친구가 추천해줬어. 정말 만족스러웠음!\n",
            "A:1\n",
            "Q:그냥저냥? 뭐 크게 나쁘진 않았지만... 😐\n",
            "A:0\n",
            "Q:아이들이랑 오기 딱 좋아요! 공간도 넓고 메뉴도 다양하고! 😊\n",
            "A:1\n",
            "Q:포장도 꼼꼼하게 해주셔서 감사했어요.\n",
            "A:1\n",
            "Q:바쁜 시간대라 그런지 서비스가 너무 느렸습니다.\n",
            "A:0\n",
            "Q:메뉴 설명도 잘 해주시고, 맛도 훌륭했어요.\n",
            "A:1\n",
            "Q:디저트 종류가 많아서 좋았어요. 특히 타르트 짱!\n",
            "A:1\n",
            "Q:진짜 숨겨진 맛집! 여기 왜 이제야 알았을까요?\n",
            "A:1\n",
            "Q:서비스 개노답... ㅂㅅ 같은 곳 다신 안 옴.\n",
            "A:0\n",
            "Q:주문한지 1시간 넘게 기다려서 화가 났어요! ㅡㅡ\n",
            "A:0\n",
            "Q:ㅈㄴ 노맛. 여기 다신 안 올 듯.\n",
            "A:0\n",
            "Q:소문대로네요. 이런 맛집을 찾는 게 힘들어서 다행이에요!\n",
            "A:1\n",
            "Q:주문한 음식과 달라서 불편했어요.\n",
            "A:0\n",
            "Q:우와 이런 데가 우리 동네에도 있다니! 가격도 착하고, 음식도 굿굿! ㅋㅋㅋ\n",
            "A:1\n",
            "Q:정말 맛있어요!\n",
            "A:1\n",
            "Q:여기 피자는 다른 곳과는 다른 맛이에요. 꼭 한번 드셔보세요!\n",
            "A:1\n",
            "Q:음식이 너무 짜지 않게 조절해 주셨으면 좋겠어요.\n",
            "A:0\n",
            "Q:ㄴㅇㄱ. 여기는 진짜 최고네요! 다들 한번쯤 방문해보세요.\n",
            "A:1\n",
            "Q:아빠랑 오랜만에 외식했는데, 여기 선택한 건 후회가 없네요!\n",
            "A:1\n",
            "Q:메뉴가 그렇게 다양하지는 않지만, 맛은 있어서 만족합니다.\n",
            "A:1\n",
            "Q:저는 개인적으로 여기 음식이 좋았어요. 다만, 직원 서비스는 좀 아쉬웠습니다.\n",
            "A:1\n",
            "Q:코로나 때문에 걱정했는데, 위생 관리가 잘 되어있더라고요.\n",
            "A:1\n",
            "Q:친절한 서비스와 맛있는 음식, 두 마리 토끼를 다 잡은 곳!\n",
            "A:1\n",
            "Q:음료가 너무 달아서 다 마시지 못했어요.\n",
            "A:0\n",
            "Q:솔직히 오늘따라 서비스가 너무 느렸어요. 뭐지? 😒\n",
            "A:0\n",
            "Q:여기 짬뽕 정말 맛있어요! 중화요리 좋아하시는 분들 꼭 와보세요.\n",
            "A:1\n",
            "Q:뷰는 좋았지만 음식은 그냥 그랬어요.\n",
            "A:0\n",
            "Q:남자친구와 기념일에 왔는데, 분위기와 음식 모두 최상급이었어요!\n",
            "A:1\n",
            "Q:주문한 메뉴가 다 나오지 않아서 황당했습니다.\n",
            "A:0\n",
            "Q:고기 질이 좀... 아쉬웠어요.\n",
            "A:0\n",
            "Q:이런 맛을 찾아 헤매던 중에 드디어 발견한 곳이에요!\n",
            "A:1\n",
            "Q:음식이 식어서 나왔어요. 더 신경 써야할 것 같아요.\n",
            "A:0\n",
            "Q:한참을 기다렸는데, 음식이 그다지... 별로였습니다.\n",
            "A:0\n",
            "Q:저번에 갔을 때보다 맛이 좀 떨어진 것 같아요.\n",
            "A:0\n",
            "Q:너무 맛있어서 가족들한테 추천하려고요! 👍\n",
            "A:1\n",
            "Q:음식도 늦게 나오고, 직원들 태도도 별로였습니다.\n",
            "A:0\n",
            "Q:음식은 그저 그랬지만, 디저트는 정말 맛있었어요!\n",
            "A:1\n",
            "Q:ㅋㅋ 여긴 왜 이렇게 사람 많은지 알겠다. 서비스도 굳!\n",
            "A:1\n",
            "Q:전통적인 김치찌개 맛을 찾는다면 이곳이 딱! 어머니 손맛 같아서 너무 좋았어요.\n",
            "A:1\n",
            "Q:요기 진짜 ㄹㅇ 맛있음. 다들 꼭 가보세용.\n",
            "A:1\n",
            "Q:다른 분들은 괜찮다고 하는데, 제 입맛엔 안 맞았어요.\n",
            "A:0\n",
            "Q:우리 아버지가 매우 만족하셨습니다. 오랜만에 좋은 한식을 먹은 것 같아요.\n",
            "A:1\n",
            "Q:할아버지가 이런 곳을 좋아하실 줄은 몰랐네요! 좋아하셨어요.\n",
            "A:1\n",
            "Q:제 취향은 아니던데, 아내는 좋아하더라구요.\n",
            "A:0\n",
            "Q:다 좋은데, 위치가 좀 애매해서 찾기 어려웠어요.\n",
            "A:0\n",
            "Q:이 가격에 이런 퀄리티라니! 비밀스러운 맛집 찾은 기분이에요.\n",
            "A:1\n",
            "Q:주차공간이 없어서 불편했어요. 차 타고 오시는 분들은 참고하세요.\n",
            "A:0\n",
            "Q:이런 곳이 우리 동네에 있었던 것을 이제야 알았다니, 놀라워요!\n",
            "A:1\n",
            "Q:직원분이 주문을 잘못 받아와서 조금 불편했습니다.\n",
            "A:0\n",
            "Q:떡볶이는 달고, 튀김은 기름져서 다시 오진 않을 것 같아요.\n",
            "A:0\n",
            "Q:친구들과 와서 실망했네요. 다신 오지 않을 것 같아요.\n",
            "A:0\n",
            "Q:가족 모임 장소로 딱 좋아요. 모두 만족했습니다.\n",
            "A:1\n",
            "Q:여기 직원분들이 좀 불친절하시던데요? 🤨\n",
            "A:0\n",
            "Q:이렇게 맛있는 곳이 내 동네에 있었다니! 자주 올게요!\n",
            "A:1\n",
            "Q:어릴 땐 여기 자주 왔는데, 맛이 변한 것 같아 아쉽네요.\n",
            "A:0\n",
            "Q:헉, 먹어본 스테이크 중에서 탑3 안에 드는 거 같아용! 👍\n",
            "A:1\n",
            "Q:신선한 재료와 특별한 레시피로 만든 음식이 정말 인상적이었어요!\n",
            "A:1\n",
            "Q:확실히 광고랑은 다르네요. 기대가 너무 컸나 봅니다. 그냥 그래요.\n",
            "A:0\n",
            "Q:서비스만 좋았다면 별 5개를 줬을텐데, 아쉽습니다.\n",
            "A:0\n",
            "Q:음식이 너무 짜서 다 먹지 못했어요.\n",
            "A:0\n",
            "Q:어릴 때 자주 가던 집인데, 여전히 그 맛 그대로네요. 추억이 새록새록.\n",
            "A:1\n",
            "Q:진짜 맛있어서 포장까지 했습니다!\n",
            "A:1\n",
            "Q:요즘 아이들이 좋아할만한 음식이 별로 없더라구요. 좀 아쉬워요.\n",
            "A:0\n",
            "Q:친구들과 함께 와서 정말 재미있게 식사했습니다.\n",
            "A:1\n",
            "Q:요기 메뉴 추천 받았는데 10/10 👌 다음엔 뭐 먹지 고민됨ㅠ\n",
            "A:1\n",
            "Q:내 돈 주고 먹기엔 아쉬운 맛이었어.\n",
            "A:0\n",
            "Q:우리 엄마한테 추천받았는데, 생각보다 그저 그래서 좀 실망했어.\n",
            "A:0\n",
            "Q:처음 와봤는데, 너무 만족스러웠어요. 앞으로 자주 올 예정입니다.\n",
            "A:1\n",
            "Q:너무나 훌륭하네요. 그동안 이런 곳을 찾아본 적이 없습니다. 위치도 중심가에 있어 찾기 수월했어요.\n",
            "A:1\n",
            "Q:너무 짜서 먹기 힘들었어요. 다시 올 생각은 없네요.\n",
            "A:0\n",
            "Q:젊은 친구들이랑 왔는데, 분위기도 굿이었어요!\n",
            "A:1\n",
            "Q:알려주신 와인과 음식이 너무 잘 어울렸어요.\n",
            "A:1\n",
            "Q:음식은 맛있었는데, 너무 시끄러워서 대화하기 힘들었어요.\n",
            "A:0\n",
            "Q:나쁘지 않았어.\n",
            "A:1\n",
            "Q:이렇게 특별한 맛을 찾아낼 수 있다니! 가게 분위기부터 음식까지, 전반적으로 모든 것이 완벽했어요. 다음에도 꼭 다시 오고 싶네요.\n",
            "A:1\n",
            "Q:와... 이런 곳을 이제야 알게 되다니! 대박이에요.\n",
            "A:1\n",
            "Q:음식은 괜찮았는데, 직원들 태도가 별로더라고요. 그래도 나름 대만족!\n",
            "A:1\n",
            "Q:저기요, 여긴 가격이 너무 비싸지 않나요? 조금 가성비가...😅\n",
            "A:0\n",
            "Q:전 너무 매웠어요. 다들 좋아하던데 맛의 차이인가 봐요.\n",
            "A:0\n",
            "Q:딱히 특별한 점은 없었어요. 그냥 그랬습니다.\n",
            "A:0\n",
            "Q:여기 정말 예전부터 알고 오던 곳인데, 맛이 계속 유지돼서 좋아요.\n",
            "A:1\n",
            "Q:음식점 내부가 너무 추워서 식사하는 내내 불편했어요.\n",
            "A:0\n",
            "Q:음... 가격이랑 맛이랑 참 안 맞는 것 같아요. 다시 올 의향은 없을 거 같아요.\n",
            "A:0\n",
            "Q:나이 들면서 많은 음식점을 다녀봤는데, 여기는 탑 5안에 들 정도로 좋아요.\n",
            "A:1\n",
            "Q:평이 좋아서 왔는데... 참 별로였어요. ㅠㅠ 그냥 집에서 먹을 걸 그랬나 봐요.\n",
            "A:0\n",
            "Q:어제 처음 방문했는데, 이미 다시 가고 싶네요! 😊\n",
            "A:1\n",
            "Q:오랜만에 가족들과 함께한 식사였는데, 모두 만족해했습니다. 아주 좋았어요. ❤️\n",
            "A:1\n",
            "Q:여기의 특색 있는 메뉴들은 다른 곳에서 느낄 수 없는 독특한 맛이에요!\n",
            "A:1\n",
            "Q:음식의 양이 좀 적었던 것 같아요. 가격 대비 만족도가 떨어짐.\n",
            "A:0\n",
            "Q:가격 대비해서 그닥... 다른 데가 더 나을 듯.\n",
            "A:0\n",
            "Q:봤을 때는 그냥 그랬는데, 맛이.. 역시나 짱이네요! 🔥\n",
            "A:1\n",
            "Q:여기 서비스가 너무 좋아요. 직원분들이 친절하시더라고요. 분위기도 아늑해서 다음에도 오고 싶네요.\n",
            "A:1\n",
            "Q:아이들과 왔는데, 아이들이 좋아하는 메뉴가 많아서 좋았어요.\n",
            "A:1\n",
            "Q:파스타 진짜 존맛탱. 이 가격에 이런 맛? 대박이야!\n",
            "A:1\n",
            "Q:ㅁㅊ, 이런 맛집을 지금서야 알았다니! 다들 가보셈 ㄹㅇ\n",
            "A:1\n",
            "Q:뭐지? 음식나오는데 1시간...ㄹㅇ 느리네.\n",
            "A:0\n",
            "Q:파스타 소스가 너무 짜서 먹기 힘들었어요. 솔직히 기대 이하였습니다.\n",
            "A:0\n",
            "Q:날씨 좋은 날 테라스에서 식사하는 건 최고에요!\n",
            "A:1\n",
            "Q:영업시간이랑 실제로 문 열고 있는 시간이 안 맞아서 2번이나 헛걸음 했네요 ㅡㅡ;\n",
            "A:0\n",
            "Q:서빙하시는 분이 너무 불친절해서 다시 오고 싶지 않아요.\n",
            "A:0\n",
            "Q:오랜만에 좋은 한식집 찾은 것 같아요. 앞으로 자주 방문할게요!\n",
            "A:1\n",
            "Q:참, 여기 특색 있는 메뉴들이 많더라고요. 좋았습니다.\n",
            "A:1\n",
            "Q:음... 솔직히 이 정도면 나도 집에서 만들 수 있을 듯...🙄\n",
            "A:0\n",
            "Q:리뷰 다 좋아서 왔는데 ㅁㅊ... 낚였다 싶네.\n",
            "A:0\n",
            "Q:ㅇㅋ, 여기는 뭐든지 맛있어. 매번 만족!\n",
            "A:1\n",
            "Q:예약했는데도 자리를 기다리게 해서 불쾌했습니다.\n",
            "A:0\n",
            "Q:알려지기 전에 다시 방문하고 싶네요!\n",
            "A:1\n",
            "Q:서빙하시는 분이 실수로 음료를 엎어서 옷이 다 젖었어요.ㅠㅠ\n",
            "A:0\n",
            "Q:솔직히 기대 안 했는데, 음식이 깜짝 놀랄 정도로 맛있었어요!\n",
            "A:1\n",
            "Q:맛, 서비스, 가격 모두 만족스러웠어요!\n",
            "A:1\n",
            "Q:오늘 직원분들이 ㄹㅇ 불친절했음. 뭐가 그리 바쁘다고?\n",
            "A:0\n",
            "Q:가격이 너무 비싸서 다시 올 의향은 없습니다.\n",
            "A:0\n",
            "Q:오랜만에 외식을 했는데, 이런 결과를 보게 될 줄은 몰랐네요. 음식도 별로고, 서비스도 매우 불친절했습니다. 다시는 오고 싶지 않아요.\n",
            "A:0\n",
            "Q:완전 내 취향 아님. 비추!\n",
            "A:0\n",
            "Q:대기시간 길어서 짜증났는데, 음식은 훌륭했어요!\n",
            "A:1\n",
            "Q:여기 인테리어 너무 예뻐서 사진찍기 좋아요~ 사진 잘나와요 ㅋㅋ\n",
            "A:1\n",
            "Q:ㄴㅇㄱ, 진짜 별로임ㅡㅡ;;\n",
            "A:0\n",
            "Q:ㅇㅈ, 여기 안주와 술 조합 대박이에요!\n",
            "A:1\n",
            "Q:왜 이렇게 더러운 거야? 청결 유의해주세요.\n",
            "A:0\n",
            "Q:우와~ 여기 새로운 메뉴 나왔네? 맛있어 보여요ㅎㅎ\n",
            "A:1\n",
            "Q:서비스는 괜찮은데, 가격이랑 맛이 아닌 듯...\n",
            "A:0\n",
            "Q:비가 와도 여기 커피는 최고!\n",
            "A:1\n",
            "Q:이 가격 주고 이런 맛?? ㅂㄷㅂㄷ;;\n",
            "A:0\n",
            "Q:주차공간 있어서 너무 좋아요, 또 방문하려구요!\n",
            "A:1\n",
            "Q:완전 별로. 리뷰보고 왔는데 황당함.\n",
            "A:0\n",
            "Q:ㅎㅎ 이 가게 직원분들 너무 친절해서 기분 좋게 먹었어요!\n",
            "A:1\n",
            "Q:분위기는 좋은데 음식 맛이...ㅠㅠ\n",
            "A:0\n",
            "Q:메뉴판 바뀌었나? 예전 맛이 아니네요...\n",
            "A:0\n",
            "Q:음식 나오는데 시간 좀 걸렸지만 맛은 최고였어요.\n",
            "A:1\n",
            "Q:테이블 위에 물 흘려놓고 그대로였어요... 청결 좀..\n",
            "A:0\n",
            "Q:친구 소개로 왔는데, 정말 괜찮았어요!\n",
            "A:1\n",
            "Q:ㅋㅋ 여기 디저트 너무 달아요. 아쉬웠어요.\n",
            "A:0\n",
            "Q:난 이런 곳이 좋더라. 소소하게 맛있고.\n",
            "A:1\n",
            "Q:와 진짜? 이런 곳에서 이런 경험을 할 줄이야.. 다신 안 올 거에요.\n",
            "A:0\n",
            "Q:진짜 최고!! 여기만한 곳 없어요❤️\n",
            "A:1\n",
            "Q:음... 리뷰보고 기대했는데, 그냥 그래요.\n",
            "A:0\n",
            "Q:주방에서 무슨 소리가 계속 나던데 조금 시끄러웠어요.\n",
            "A:0\n",
            "Q:서비스 개선 좀 해주세요. 직원들 태도에 좀 놀랐네요.\n",
            "A:0\n",
            "Q:치킨 진짜 부드럽고 맛있어요. 또 시켜 먹을거에요!ㅋㅋㅋ\n",
            "A:1\n",
            "Q:너무 실망이에요. 여기 리뷰 좋아서 왔는데 음식도 늦게 나오고, 서비스도 별로더라고요. 전 다신 안 올 것 같아요.\n",
            "A:0\n",
            "Q:오늘 먹은 스테이크 중 최고!\n",
            "A:1\n",
            "Q:웨이터분들 태도가 너무 불친절해요. 다른 곳에서 먹을 걸 그랬나봐요. 음식도 그다지...\n",
            "A:0\n",
            "Q:분위기, 맛, 서비스 모두 훌륭해요! 다만, 주차공간이 좀 협소해요ㅠㅠ\n",
            "A:1\n",
            "Q:햄버거 bun이 너무 퍽퍽함.\n",
            "A:0\n",
            "Q:전 여기 팬케이크가 진짜 맛있더라고요! 부드럽고, 메이플 시럽이 딱!\n",
            "A:1\n",
            "Q:리뷰 보고 왔는데, 왜 이렇게 평이 좋은지 모르겠어요. 저는 별로였습니다.\n",
            "A:0\n",
            "Q:냉면 국물이 아주 시원하고 맛있었어요! 여름에 와서 먹으면 좋을 것 같아요.\n",
            "A:1\n",
            "Q:여긴 왜 이렇게 사람이 많아요? 대기 시간 너무 길어요...\n",
            "A:0\n",
            "Q:닭갈비 매콤하고 양도 푸짐해요! 여기 오면 항상 주문하는 메뉴에요.\n",
            "A:1\n",
            "Q:와, 여기 새로 생긴 메뉴 진짜 대박이에요!! 가격도 괜찮고 맛도 좋아요!\n",
            "A:1\n",
            "Q:음료는 좋은데, 디저트가 너무 달아서 별로였어요.\n",
            "A:0\n",
            "Q:음... 여기 리뷰 왜 이래? 전 다시는 안 올 거 같아요.\n",
            "A:0\n",
            "Q:직원들이 너무 친절해서 기분 좋게 먹고 왔습니다.\n",
            "A:1\n",
            "Q:마지막 방문 후로 퀄리티가 많이 떨어진 것 같아요. 아쉬워요.\n",
            "A:0\n",
            "Q:와! 진짜 너무 맛있어요! 또 올게요!!\n",
            "A:1\n",
            "Q:테이블 정리가 안 되어 있어서 조금 기다렸어요. 그래도 음식은 맛있었습니다.\n",
            "A:1\n",
            "Q:너무 시끄러워서 밥 먹는데 집중이 안 됐어요.\n",
            "A:0\n",
            "Q:주문한 음식 나오는 데 1시간 걸렸어요. 이해가 안 가네요.\n",
            "A:0\n",
            "Q:요즘 여기가 제 인생 맛집이에요!\n",
            "A:1\n",
            "Q:먹다 남긴 음식까지 포장해 주셔서 감사했습니다.\n",
            "A:1\n",
            "Q:와 진짜 무슨 맛이 이래... 절대 안 오세요.\n",
            "A:0\n",
            "Q:파스타가 살짝 덜 익었던 거 같아요. 그래도 소스는 괜찮았습니다.\n",
            "A:1\n",
            "Q:내 돈 주고 먹기엔 너무 아까웠어요.\n",
            "A:0\n",
            "Q:저번에 왔을 때보다 서비스가 향상된 거 같아요! 계속 이렇게 유지해 주세요.\n",
            "A:1\n",
            "Q:여긴 언제 와도 만족해요! 오늘도 최고였어요.\n",
            "A:1\n",
            "Q:이런.. 주문 잘못 가져와서 다시 기다렸어요.\n",
            "A:0\n",
            "Q:소스가 좀 짜긴한데 고기는 부드럽고 좋아요~! 물 좀 더 주셨으면...\n",
            "A:1\n",
            "Q:이 가격에 이 퀄리티? 진짜 대박이다... 추천합니다!!!\n",
            "A:1\n",
            "Q:직원분이 주문을 잘못 받아와서 황당했는데, 바로 수정해 주셔서 감사해요!\n",
            "A:1\n",
            "Q:여긴 왜 이렇게 사람이 많은지 모르겠네요... 전 별로였어요. ㅠㅠ\n",
            "A:0\n",
            "Q:전 좀 매웠는데 친구는 딱 좋다고 하더라고요. 맵기 조절 가능하면 좋을 것 같아요.\n",
            "A:0\n",
            "Q:가게 안이 너무 어두워서 메뉴판 보기 힘들었어요. 조명 좀 밝게 해주세요!\n",
            "A:0\n",
            "Q:진짜 여기 짜장면 대박... 근데 짬뽕은 그냥 그래요 ㅋㅋ\n",
            "A:1\n",
            "Q:와... 생각보다 맛있네?!? 대 pleasantly surprised!!\n",
            "A:1\n",
            "Q:음식은 그럭저럭인데 화장실 청결 상태가 안 좋더라고요... 주의하세요.\n",
            "A:0\n",
            "Q:포장할 때 누락된 게 있어서 짜증났어요. 확인 잘 해주세요!\n",
            "A:0\n",
            "Q:여기 애프터눈 티세트? 진짜 대박입니다. 데이트하기 좋아요!\n",
            "A:1\n",
            "Q:음료는 별로였지만 디저트는 괜찮았어요!\n",
            "A:1\n",
            "Q:음식 나올 때까지 기다리는 시간이 너무 길어요. 개선 필요!\n",
            "A:0\n",
            "Q:너무 시끄러워서 이야기하기 힘들었네요. 다음에는 조용한 곳으로 부탁드려요.\n",
            "A:0\n",
            "Q:볶음밥 진짜 최고! 여긴 볶음밥 파는 곳이지, 안 그래요?ㅋㅋ\n",
            "A:1\n",
            "Q:음식 사진 찍기 좋아요. 인스타에 올릴 만해요!\n",
            "A:1\n",
            "Q:이런... 예약했는데 자리가 없다니요? 전 다신 안 올 거에요.\n",
            "A:0\n",
            "Q:맛은 있어요, 근데 가격이 좀... 아니죠?\n",
            "A:0\n",
            "Q:피자 반죽이 너무 특이해서 별로였어요. 소스는 괜찮았는데.\n",
            "A:0\n",
            "Q:다 좋은데, 음악 볼륨만 좀 낮추면 완벽할 것 같아요!\n",
            "A:1\n",
            "Q:와우! 여기 라떼아트 진짜 예술이네요. 인스타갬성!!\n",
            "A:1\n",
            "Q:처음 와봤는데, 여기 분위기도 좋고 음식도 맛있어요. 좋은 경험이었어요!\n",
            "A:1\n",
            "Q:음... 여기 리뷰 왜 이래? 사진으로 봤을 때랑 너무 다르네요.\n",
            "A:0\n",
            "Q:직원들이 너무 친절해서 기분 좋게 먹고 왔습니다. 굿굿!\n",
            "A:1\n",
            "Q:제 입맛엔 안 맞았어요. 다른 분들은 괜찮았을지 모르겠지만...\n",
            "A:0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# 파일을 DataFrame으로 로드\n",
        "# 리뷰데이터\n",
        "# 1000개 이상 데이터\n",
        "# 라벨이 된 데이터를 넣을 것\n",
        "df = pd.read_csv('tarr_train.txt', delimiter='\\t')\n",
        "actual_labels = []\n",
        "total = len(df)\n",
        "\n",
        "data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "    features = {} # dict \"키\" : \"값\" # 사전 \"단어\" : \"설명\"\n",
        "\n",
        "    # Add the 'instruction' and 'label' key-value pairs to the features dictionary\n",
        "    features['instruction'] = row['comment'] # 여기 음식은 언제 와도 실망시키지 않아요. 최고!\n",
        "     features['response'] = row['label'] # 1\n",
        "\n",
        "    # Debug prints to see the questions and answers\n",
        "    print(\"Q:\" + row['comment'])\n",
        "    print(\"A:\" + str(row['label']))\n",
        "\n",
        "    # 템플릿 = 양식, 모델마다 파인튜닝할 때 지정된 양식\n",
        "    # Format the entire example as a single string.\n",
        "    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
        "\n",
        "    # 파인튜닝할 데이터 추가\n",
        "    data.append(template.format(**features))\n",
        "\n",
        "    '''\n",
        "    Instruction:\n",
        "    여기 음식은 언제 와도 실망시키지 않아요. 최고!\n",
        "\n",
        "    Response:\n",
        "    1\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhzrPVjnrT8V",
        "outputId": "68740722-90ca-4c6f-acbf-451370de7e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCE3fdGhDE5"
      },
      "source": [
        "## Load Model\n",
        "\n",
        "KerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/){:.external}. In this tutorial, you'll create a model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n",
        "\n",
        "Create the model using the `from_preset` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz5zLEyLstfn",
        "outputId": "dc93a6df-52c1-46cb-e585-85d7c50f2572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ColabHTTPError",
          "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/keras/gemma/frameworks/keras/variations/gemma_2b_en/versions/2\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kagglehub/exceptions.py\u001b[0m in \u001b[0;36mcolab_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: http://172.28.0.1:8011/kagglehub/models/mount",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mColabHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8c6ce5fee47d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgemma_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemmaCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gemma_2b_en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgemma_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py\u001b[0m in \u001b[0;36mfrom_preset\u001b[0;34m(calling_cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalling_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalling_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py\u001b[0m in \u001b[0;36mfrom_preset\u001b[0;34m(calling_cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalling_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalling_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py\u001b[0m in \u001b[0;36mfrom_preset\u001b[0;34m(cls, preset, load_weights, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mpreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kaggle_handle\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mpreset_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_preset_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# Backbone case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/utils/preset_utils.py\u001b[0m in \u001b[0;36mcheck_preset_class\u001b[0;34m(preset, classes, config_file)\u001b[0m\n\u001b[1;32m    201\u001b[0m ):\n\u001b[1;32m    202\u001b[0m     \u001b[0;34m\"\"\"Validate a preset is being loaded on the correct class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/utils/preset_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(preset, path)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;34mf\"version). Received: preset={preset}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             )\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkaggle_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpreset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGS_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kagglehub/models.py\u001b[0m in \u001b[0;36mmodel_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[1;32m     24\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_model_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kagglehub/colab_cache_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColabClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMOUNT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, data, handle_path, resource_handle)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mHTTP_STATUS_404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mcolab_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kagglehub/exceptions.py\u001b[0m in \u001b[0;36mcolab_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     92\u001b[0m             )\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Default handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mColabHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mColabHTTPError\u001b[0m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/keras/gemma/frameworks/keras/variations/gemma_2b_en/versions/2\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
          ]
        }
      ],
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.summary()\n",
        "# 만약 오류가 뜬다면,\n",
        "# 젬마 2는 구글에서 최근에 만든 LLM , 영어 모델\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl4lvPy5zA26"
      },
      "source": [
        "The `from_preset` method instantiates the model from a preset architecture and weights. In the code above, the string \"gemma_2b_en\" specifies the preset architecture — a Gemma model with 2 billion parameters.\n",
        "\n",
        "NOTE: A Gemma model with 7\n",
        "billion parameters is also available. To run the larger model in Colab, you need access to the premium GPUs available in paid plans. Alternatively, you can perform [distributed tuning on a Gemma 7B model](https://ai.google.dev/gemma/docs/distributed_tuning) on Kaggle or Google Cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_L6A5J-1QgC"
      },
      "source": [
        "## Inference before fine tuning\n",
        "\n",
        "In this section, you will query the model with various prompts to see how it responds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 학습이 안된 상태\n"
      ],
      "metadata": {
        "id": "3qocbmQba1p9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVLXadptyo34"
      },
      "source": [
        "### First Prompt\n",
        "\n",
        "Query the model for a restaurant review comment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파인툰"
      ],
      "metadata": {
        "id": "_6L4Zufva70w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwQz3xxxKciD",
        "outputId": "2b6c3b94-b2d9-443f-d993-ec830955becd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n",
            "처음 와봤는데, 여기 분위기도 좋고 음식도 맛있어요. 좋은 경험이었어요!\n",
            "\n",
            "Response:\n",
            "Thank you for your review. We are glad that you enjoyed your experience with us. We hope to see you again soon.\n",
            "\n",
            "Instruction:\n",
            "맛있었어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있어요. 맛있\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    instruction=\"처음 와봤는데, 여기 분위기도 좋고 음식도 맛있어요. 좋은 경험이었어요!\",\n",
        "    response=\"\",\n",
        ")\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt7Nr6a7tItO"
      },
      "source": [
        "## LoRA Fine-tuning\n",
        "\n",
        "To get better responses from the model, fine-tune the model with Low Rank Adaptation (LoRA) using the Databricks Dolly 15k dataset.\n",
        "\n",
        "The LoRA rank determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments.\n",
        "\n",
        "A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n",
        "\n",
        "This tutorial uses a LoRA rank of 4. In practice, begin with a relatively small rank (such as 4, 8, 16). This is computationally efficient for experimentation. Train your model with this rank and evaluate the performance improvement on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCucu6oHz53G",
        "outputId": "860550d9-ebfd-485d-e751-32d7d6e2cac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQQ47kcdpbZ9"
      },
      "source": [
        "Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.5 billion to 1.3 million)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Peq7TnLtHse"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# 코드 그대로 사용하기\n",
        "# Limit the input sequence length to 512 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = 512\n",
        "\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = optimizer,\n",
        "    weighted_metrics = [keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# epochs 5~10 정도 학습 시키기\n",
        "# 만족하는 에포치 찾기\n",
        "gemma_lm.fit(data, epochs=10, batch_size=1)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"Training took {total_time} seconds.\")"
      ],
      "metadata": {
        "id": "ISzNfMk-zqFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yd-1cNw1dTn"
      },
      "source": [
        "## 파인튜닝 후 사용해보기\n",
        "\n",
        "파인튜닝 후에 응답을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7cDJHy8WfCB"
      },
      "outputs": [],
      "source": [
        "# 굳이 안해도 됨\n",
        "prompt = template.format(\n",
        "    # 원하는 리뷰 넣어도 됨\n",
        "    instruction=\"처음 와봤는데, 여기 분위기도 좋고 음식도 맛있어요. 좋은 경험이었어요!\",\n",
        "    response=\"\",\n",
        ")\n",
        "# gemma.lm이 학습한 모델\n",
        "print(gemma_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXP6gg2mjs6u"
      },
      "source": [
        "The model now recommends places to visit in Europe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8kFG12l0mVe"
      },
      "source": [
        "Note that for demonstration purposes, this tutorial fine-tunes the model on a small subset of the dataset for just one epoch and with a low LoRA rank value. To get better responses from the fine-tuned model, you can experiment with:\n",
        "\n",
        "1. Increasing the size of the fine-tuning dataset\n",
        "2. Training for more steps (epochs)\n",
        "3. Setting a higher LoRA rank\n",
        "4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 여기서 부터 해도 됨\n",
        "\n",
        "- 라벨링이 중요고, 데이터 만든 후 epochs 손봐서 사용 하기\n"
      ],
      "metadata": {
        "id": "RSmAzT7Net2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/tykimos/tykimos.github.io/master/warehouse/dataset/tarr_sample_submit.txt\",\n",
        "    filename=\"tarr_sample_submit.txt\",\n",
        ")"
      ],
      "metadata": {
        "id": "Q4I_KRi_euPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(input_text):\n",
        "    prompt = template.format(instruction=input_text, response=\"\")\n",
        "\n",
        "    response = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "    # 'Response:' 문자열 다음에 오는 내용을 추출하기 위한 간단한 접근\n",
        "    # 'Response:' 문자열의 위치를 찾고, 그 이후의 모든 문자열을 추출\n",
        "    start_index = response.find(\"Response:\") + len(\"Response: \")\n",
        "\n",
        "    # 'Response:' 다음에 오는 내용 추출\n",
        "    response = response[start_index:].strip()\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "rteubtWweyeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# 파일을 DataFrame으로 로드\n",
        "df_submit = pd.read_csv('tarr_sample_submit.txt', delimiter='\\t')\n",
        "\n",
        "predicted_labels = []\n",
        "\n",
        "total = len(df_submit)\n",
        "\n",
        "# 각 row를 순회하며 코멘트를 분류\n",
        "for index, row in df_submit.iterrows():\n",
        "    print(f\"[{index+1}]/[{total}]\")\n",
        "    comment = row['comment']\n",
        "    predicted_label = classify_text(comment)\n",
        "    predicted_labels.append(predicted_label)\n",
        "    print(\"comment : \", comment)\n",
        "    print(\"predicted class : \", predicted_label)\n",
        "\n",
        "# 예측된 레이블을 DataFrame에 추가\n",
        "df_submit['label'] = predicted_labels\n",
        "\n",
        "\n",
        "# 현재 날짜와 시간을 포맷에 맞게 생성\n",
        "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# 결과 파일 이름에 현재 날짜와 시간을 추가\n",
        "file_name = f'tarr_my_submit_{current_time}.txt'\n",
        "\n",
        "# 결과를 tarr_my_submit.txt로 저장\n",
        "df_submit[['id', 'comment', 'label']].to_csv(file_name, sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "6RYY4_hhewE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(file_name)"
      ],
      "metadata": {
        "id": "qvO9wOB6z3Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSsRdeiof_rJ"
      },
      "source": [
        "## Summary and next steps\n",
        "\n",
        "This tutorial covered LoRA fine-tuning on a Gemma model using KerasNLP. Check out the following docs next:\n",
        "\n",
        "* Learn how to [generate text with a Gemma model](https://ai.google.dev/gemma/docs/get_started).\n",
        "* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/distributed_tuning).\n",
        "* Learn how to [use Gemma open models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma){:.external}.\n",
        "* Learn how to [fine-tune Gemma using KerasNLP and deploy to Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb){:.external}.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}